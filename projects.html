<!doctype html>

<html lang="en">
<head>
  <meta charset="utf-8">

  <title>Anish Krishnan</title>
  <meta name="description" content="Anish's Personal Website">
  <meta name="author" content="Anish Krishnan">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link href="https://fonts.googleapis.com/css?family=Nanum+Gothic:400,700,800|Roboto" rel="stylesheet">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="css/styles.css?v=1.0">


</head>

<body>
  <script src="js/scripts.js"></script>

  <h1>Anish Krishnan</h1>
  <h4>Carnegie Mellon University</h4>
  <h5>Information Systems and Computer Science</h5>
  <h5><a href="mailto:anishkrishnan@cmu.edu" class="email">anishkrishnan@cmu.edu</a></h5>
  <br />

  <a class="btn btn-info" href="index.html" role="button">Home</a>
  <a class="btn btn-outline-info" href="projects.html" role="button">Projects</a>
  <a class="btn btn-info" href="files/resume.pdf" role="button">Resume</a>
  <a class="btn btn-info" href="https://github.com/anish-krishnan" role="button">Github</a>
  <br /><br />


  <a href="https://devpost.com/anishKrishnan"><img src="images/devpost.png" class="rounded float-left img-fluid" alt="..." width="120"></a><br /><br />


  <h3>Projects</h3>
  <br />

    <table style="width:80%;" class="hide-mobile">
    <tr>
      <th style="width:150px"></th>
      <th style="width:600px"></th>
      <th style="width:150px"></th>
    </tr>

    <tr>
      <td>
        <img src="images/echo.png" class="rounded float-left img-fluid" alt="..." width="100">
      </td>
      <td><b>Echo: Penn Apps Winner (2nd Overall and won 5 other awards)</b></td>
      <td>02/19</td>
    </tr>
    <tr class="spaceUnder">
      <td></td>
      <td>
        <p style="width: 90%">
          Echo is an intelligent, environment-aware smart cane that acts as assistive tech for the visually or mentally impaired.<br /><br />

          Using cameras, microphones, and state of the art facial recognition, natural language processing, and speech to text software, Echo is able recognize familiar and new faces allowing patients to confidently meet new people and learn more about the world around them. Echo also has a button that, when pressed, will contact the authorities- this way, if the owner is in danger, help is one tap away.<br />
        </p>
        <a href="https://devpost.com/software/pennapps2018-l4m37i" style="color: #24a1b6;">DevPost Link</a><br/><br/>
        <iframe width="80%" height="200px" src="https://www.youtube.com/embed/oz_wIluf-EY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td>
      <td></td>
    </tr>

    <tr>
      <td>
        <img src="images/syne.png" class="rounded float-left img-fluid" alt="..." width="90">
      </td>
      <td><b>Syne: HackCMU Winner</b></td>
      <td>09/18</td>
    </tr>
    <tr class="spaceUnder">
      <td></td>
      <td>
        <p style="width: 90%">
          Syne is a tensorflow-based sign language processing system that allows mute people to efficiently communicate with the outside world.<br /><br />

          This system gathers (x, y, z) coordinates of each of the 15 joints in our hands using a leap motion sensor, which we then map to 3D vectors relative to the position of our palm. These vectors were normalized in order to ensure that the size of the hand doesn’t affect the accuracy of the system. Our neural network takes in the 45 data points, and, using three dense hidden layers, categorizes the hand gesture as one of the 26 letters. This model was trained with thousands of readings for each letter, and while training, received a validation accuracy of over 99%.<br />
        </p>
        <a href="https://devpost.com/software/syne" style="color: #24a1b6;">DevPost Link</a><br/><br/>
        <iframe width="80%" height="200px" src="https://www.youtube.com/embed/UZ5P2KzhZBg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


      </td>
      <td></td>
    </tr>

    <tr>
      <td>
        <img src="images/syft.png" class="rounded float-left img-fluid" alt="..." width="80">
      </td>
      <td><b>Syft: CMU Tartan Hacks Winner</b></td>
      <td>02/18</td>
    </tr>
    <tr class="spaceUnder">
      <td></td>
      <td>
        <p style="width: 90%">
          Syft lets you practice speeches on virtual stages (using VR) so that you can overcome stage fright. It even uses Google's Natural Language Processing (NLP) to analyze your speech and give you feedback!<br/><br/>

          Using Microsoft Azure and Bing's Speech to Text API alongside Google's NLP API, Syft is able to create an aggregate of the linguistic data from the user's speech to create a more accurate evaluation of the speaker. Syft takes advantage of the power of VR to place the user in a stage with an active crowd, so that the speaker can practice his/her speech in a more accurate environment. Syft analyzes your speech in real time looking specifically for stutters, rhythm, project, vigor, and even applies sentiment analysis.<br />
        </p>
        <a href="https://devpost.com/software/syft" style="color: #24a1b6;">DevPost Link</a><br/><br/>
        <iframe width="80%" height="200px" src="https://www.youtube.com/embed/cKHWy-3XRI4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td>
      <td></td>
    </tr>

    <tr>
      <td>
        <img src="images/airdj.png" class="rounded float-left img-fluid" alt="..." width="80">
      </td>
      <td><b>Air DJ: Innovative Music Creation Platform</b></td>
      <td>11/17</td>
    </tr>
    <tr class="spaceUnder">
      <td></td>
      <td>
        <p style="width: 90%">
          Air DJ is an intuitive new method of convolving your music with your hands without the use of a keyboard or mouse. This app utilizes the Leap Motion Sensor.<br/><br/>

          Air DJ allows the user to play tracks and add various beats to a song all controlled with hand gestures.
        </p>

        <iframe width="80%" height="200px" src="https://www.youtube.com/embed/cadKvNfI1rA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

      </td>
      <td></td>
    </tr>
    </table>


    <table style="width:100%;" class="show-mobile">
      <tr>
        <th style="width:100%"></th>
      </tr>

      <tr><td>
        <img src="images/echo.png" class="rounded float-left img-fluid" alt="..." width="100">
      </td></tr>
      <tr><td>
        <b>Echo: Penn Apps Winner (2nd Overall and won 5 other awards)</b>
      </td></tr>
      <tr><td>
        02/19
      </td></tr>
      <tr class="spaceUnder"><td>
        <p style="width: 90%">
          Echo is an intelligent, environment-aware smart cane that acts as assistive tech for the visually or mentally impaired.<br /><br />

          Using cameras, microphones, and state of the art facial recognition, natural language processing, and speech to text software, Echo is able recognize familiar and new faces allowing patients to confidently meet new people and learn more about the world around them. Echo also has a button that, when pressed, will contact the authorities- this way, if the owner is in danger, help is one tap away.<br />
        </p>
        <a href="https://devpost.com/software/pennapps2018-l4m37i" style="color: #24a1b6;">DevPost Link</a><br/><br/>
        <iframe width="80%" height="100px" src="https://www.youtube.com/embed/oz_wIluf-EY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>


      <tr><td>
        <img src="images/syne.png" class="rounded float-left img-fluid" alt="..." width="100">
      </td></tr>
      <tr><td>
        <b>Syne: HackCMU Winner</b>
      </td></tr>
      <tr><td>
        09/18
      </td></tr>
      <tr class="spaceUnder"><td>
        <p style="width: 90%">
          Syne is a tensorflow-based sign language processing system that allows mute people to efficiently communicate with the outside world.<br /><br />

          This system gathers (x, y, z) coordinates of each of the 15 joints in our hands using a leap motion sensor, which we then map to 3D vectors relative to the position of our palm. These vectors were normalized in order to ensure that the size of the hand doesn’t affect the accuracy of the system. Our neural network takes in the 45 data points, and, using three dense hidden layers, categorizes the hand gesture as one of the 26 letters. This model was trained with thousands of readings for each letter, and while training, received a validation accuracy of over 99%.<br />
        </p>
        <a href="https://devpost.com/software/syne" style="color: #24a1b6;">DevPost Link</a><br/><br/>
        <iframe width="80%" height="100px" src="https://www.youtube.com/embed/UZ5P2KzhZBg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>


      <tr><td>
        <img src="images/syft.png" class="rounded float-left img-fluid" alt="..." width="100">
      </td></tr>
      <tr><td>
        <b>Syft: CMU Tartan Hacks Winner</b>
      </td></tr>
      <tr><td>
        02/18
      </td></tr>
      <tr class="spaceUnder"><td>
        <p style="width: 90%">
          Syft lets you practice speeches on virtual stages (using VR) so that you can overcome stage fright. It even uses Google's Natural Language Processing (NLP) to analyze your speech and give you feedback!<br/><br/>

          Using Microsoft Azure and Bing's Speech to Text API alongside Google's NLP API, Syft is able to create an aggregate of the linguistic data from the user's speech to create a more accurate evaluation of the speaker. Syft takes advantage of the power of VR to place the user in a stage with an active crowd, so that the speaker can practice his/her speech in a more accurate environment. Syft analyzes your speech in real time looking specifically for stutters, rhythm, project, vigor, and even applies sentiment analysis.<br />
        </p>
        <a href="https://devpost.com/software/syft" style="color: #24a1b6;">DevPost Link</a><br/><br/>
        <iframe width="80%" height="100px" src="https://www.youtube.com/embed/cKHWy-3XRI4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>


      <tr><td>
        <img src="images/airdj.png" class="rounded float-left img-fluid" alt="..." width="100">
      </td></tr>
      <tr><td>
        <b>Air DJ: Innovative Music Creation Platform</b>
      </td></tr>
      <tr><td>
        11/17
      </td></tr>
      <tr class="spaceUnder"><td>
        <p style="width: 90%">
          Air DJ is an intuitive new method of convolving your music with your hands without the use of a keyboard or mouse. This app utilizes the Leap Motion Sensor.<br/><br/>

          Air DJ allows the user to play tracks and add various beats to a song all controlled with hand gestures.
        </p>

        <iframe width="80%" height="100px" src="https://www.youtube.com/embed/cadKvNfI1rA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </td></tr>

    </table>

</body>
</html>
